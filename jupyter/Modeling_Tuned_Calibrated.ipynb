{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe43323",
   "metadata": {},
   "source": [
    "# Modelado avanzado y calibración de modelos\n",
    "\n",
    "En esta sección realizamos el **modelado avanzado** sobre el dataset limpio con *features* ya generadas (`Cleaned_Featured_Dataset.csv`).  \n",
    "Los objetivos principales son:\n",
    "\n",
    "- Probar **tres modelos de clasificación**:\n",
    "  - Regresión logística (tuneada con GridSearchCV)\n",
    "  - Random Forest (tuneado)\n",
    "  - Gradient Boosting (tuneado)\n",
    "- Utilizar **validación cruzada (CV=5)** para seleccionar los mejores hiperparámetros.\n",
    "- Comparar el desempeño de los modelos usando:\n",
    "  - Accuracy, Precision, Recall, F1, AUC\n",
    "  - Reporte de clasificación\n",
    "- Aplicar **calibración de probabilidades** (Platt / Isotónica) al mejor modelo (Random Forest) y comparar el **Brier score** y el AUC antes y después de calibrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a99d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset limpio con features...\n",
      "Filas: 10706\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report,\n",
    "    brier_score_loss\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Cargando dataset limpio con features...\")\n",
    "data = pd.read_csv(\"Cleaned_Featured_Dataset.csv\")\n",
    "print(\"Filas:\", len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8e5ed",
   "metadata": {},
   "source": [
    "## Definición de variables de entrada (X) y salida (y)\n",
    "\n",
    "A partir del dataset limpio, seleccionamos **únicamente las columnas numéricas** como variables de entrada (*features*), excluyendo explícitamente la columna objetivo `label`.\n",
    "\n",
    "Pasos en esta celda:\n",
    "\n",
    "1. Seleccionar todas las columnas numéricas.\n",
    "2. Eliminar `label` de la lista de *features*.\n",
    "3. Definir:\n",
    "   - `X`: matriz de características numéricas.\n",
    "   - `y`: variable objetivo binaria (0 = correo legítimo, 1 = fraude/phishing).\n",
    "4. Dividir el dataset en:\n",
    "   - **Train** (80%) para entrenar y hacer validación cruzada.\n",
    "   - **Test** (20%) para evaluación final.\n",
    "5. Usar `stratify=y` para mantener el mismo balance de clases en Train y Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e62102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes:\n",
      "X: (10706, 13)\n",
      "y: (10706,)\n",
      "\n",
      "Train/Test:\n",
      "X_train: (8564, 13) X_test: (2142, 13)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 1. Definir X, y\n",
    "# ===========================\n",
    "# Tomamos solo columnas numéricas y quitamos 'label'\n",
    "num_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"label\")\n",
    "\n",
    "X = data[num_cols]\n",
    "y = data[\"label\"]\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test:\")\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2a14b",
   "metadata": {},
   "source": [
    "## Función auxiliar para reporte de métricas\n",
    "\n",
    "Para evitar repetir código al evaluar cada modelo, definimos una función `print_metrics` que:\n",
    "\n",
    "- Recibe:\n",
    "  - Nombre del modelo (`nombre`)\n",
    "  - Etiquetas reales (`y_true`)\n",
    "  - Etiquetas predichas (`y_pred`)\n",
    "  - Probabilidades predichas (`y_proba`)\n",
    "- Imprime:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1\n",
    "  - AUC\n",
    "  - **Classification report** completo (por clase)\n",
    "\n",
    "Esto estandariza la comparación entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440ec9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para imprimir métricas\n",
    "def print_metrics(nombre, y_true, y_pred, y_proba):\n",
    "    print(f\"\\n=== {nombre} ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
    "    print(\"F1       :\", f1_score(y_true, y_pred))\n",
    "    print(\"AUC      :\", roc_auc_score(y_true, y_proba))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131a119",
   "metadata": {},
   "source": [
    "## Modelo 1: Regresión Logística (tuneada con GridSearchCV)\n",
    "\n",
    "En este bloque entrenamos una **Regresión Logística** y ajustamos sus hiperparámetros usando `GridSearchCV` con validación cruzada de 5 folds.\n",
    "\n",
    "- Hiperparámetros explorados:\n",
    "  - `C`: [0.01, 0.1, 1, 10] (fuerza de regularización)\n",
    "  - `penalty`: \"l2\"\n",
    "  - `solver`: [\"lbfgs\", \"liblinear\"]\n",
    "- `scoring = \"roc_auc\"` para priorizar el área bajo la curva ROC.\n",
    "- Al final:\n",
    "  - Imprimimos los **mejores parámetros**.\n",
    "  - Reportamos el **mejor AUC promedio en CV**.\n",
    "  - Evaluamos el modelo óptimo sobre el conjunto de prueba (`X_test`, `y_test`) usando la función `print_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6fbb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Entrenando Logistic Regression (GridSearchCV)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/var/data/python/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros LogReg: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Mejor AUC CV LogReg: 0.8626755738396865\n",
      "\n",
      "=== LOGISTIC REGRESSION TUNED ===\n",
      "Accuracy : 0.7642390289449112\n",
      "Precision: 0.8240887480190174\n",
      "Recall   : 0.7860922146636432\n",
      "F1       : 0.804642166344294\n",
      "AUC      : 0.8550460205789003\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70       819\n",
      "           1       0.82      0.79      0.80      1323\n",
      "\n",
      "    accuracy                           0.76      2142\n",
      "   macro avg       0.75      0.76      0.75      2142\n",
      "weighted avg       0.77      0.76      0.77      2142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 2. Modelo 1: Logistic Regression (tuneada)\n",
    "# ===========================\n",
    "log_clf = LogisticRegression(max_iter=2000)\n",
    "\n",
    "param_grid_log = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "grid_log = GridSearchCV(\n",
    "    log_clf,\n",
    "    param_grid_log,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Entrenando Logistic Regression (GridSearchCV)...\")\n",
    "grid_log.fit(X_train, y_train)\n",
    "print(\"Mejores parámetros LogReg:\", grid_log.best_params_)\n",
    "print(\"Mejor AUC CV LogReg:\", grid_log.best_score_)\n",
    "\n",
    "best_log = grid_log.best_estimator_\n",
    "y_pred_log = best_log.predict(X_test)\n",
    "y_proba_log = best_log.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print_metrics(\"LOGISTIC REGRESSION TUNED\", y_test, y_pred_log, y_proba_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1456e4",
   "metadata": {},
   "source": [
    "## Modelo 2: Random Forest (tuneado con GridSearchCV)\n",
    "\n",
    "En este bloque ajustamos un **RandomForestClassifier** usando GridSearchCV:\n",
    "\n",
    "- Hiperparámetros explorados:\n",
    "  - `n_estimators`: [100, 300]\n",
    "  - `max_depth`: [None, 10, 20]\n",
    "  - `min_samples_split`: [2, 5]\n",
    "  - `min_samples_leaf`: [1, 2]\n",
    "- Mantenemos `random_state` fijo para reproducibilidad.\n",
    "- Utilizamos nuevamente `scoring = \"roc_auc\"` y `cv = 5`.\n",
    "\n",
    "Al final:\n",
    "\n",
    "- Reportamos los parámetros óptimos.\n",
    "- Evaluamos el modelo Random Forest resultante en el conjunto de prueba.\n",
    "- Este modelo es un candidato fuerte para ser el **modelo principal** del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e475233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Entrenando RandomForest (GridSearchCV)...\n",
      "Mejores parámetros RF: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Mejor AUC CV RF: 0.9760134667750974\n",
      "\n",
      "=== RANDOM FOREST TUNED ===\n",
      "Accuracy : 0.9164332399626517\n",
      "Precision: 0.9359756097560976\n",
      "Recall   : 0.9281934996220711\n",
      "F1       : 0.932068311195446\n",
      "AUC      : 0.9727614285437415\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       819\n",
      "           1       0.94      0.93      0.93      1323\n",
      "\n",
      "    accuracy                           0.92      2142\n",
      "   macro avg       0.91      0.91      0.91      2142\n",
      "weighted avg       0.92      0.92      0.92      2142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 3. Modelo 2: RandomForest (tuneado)\n",
    "# ===========================\n",
    "rf_clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [100, 300],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    rf_clf,\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Entrenando RandomForest (GridSearchCV)...\")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(\"Mejores parámetros RF:\", grid_rf.best_params_)\n",
    "print(\"Mejor AUC CV RF:\", grid_rf.best_score_)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print_metrics(\"RANDOM FOREST TUNED\", y_test, y_pred_rf, y_proba_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abc3a3",
   "metadata": {},
   "source": [
    "## Modelo 3: Gradient Boosting (tercer método de clasificación)\n",
    "\n",
    "Como tercer modelo probamos **GradientBoostingClassifier**, un método de *boosting* que combina múltiples árboles débiles de manera secuencial.\n",
    "\n",
    "- Hiperparámetros explorados:\n",
    "  - `n_estimators`: [100, 200]\n",
    "  - `learning_rate`: [0.05, 0.1]\n",
    "  - `max_depth`: [3, 5]\n",
    "- De nuevo utilizamos:\n",
    "  - `cv = 5`\n",
    "  - `scoring = \"roc_auc\"`\n",
    "\n",
    "El objetivo es comparar su desempeño con Random Forest y Regresión Logística, y evaluar si el *boosting* aporta alguna mejora significativa en AUC y métricas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50c3f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Entrenando GradientBoosting (GridSearchCV)...\n",
      "Mejores parámetros GB: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Mejor AUC CV GB: 0.973294383851621\n",
      "\n",
      "=== GRADIENT BOOSTING TUNED ===\n",
      "Accuracy : 0.919234360410831\n",
      "Precision: 0.9369300911854104\n",
      "Recall   : 0.9319727891156463\n",
      "F1       : 0.9344448654793482\n",
      "AUC      : 0.9716751712216565\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       819\n",
      "           1       0.94      0.93      0.93      1323\n",
      "\n",
      "    accuracy                           0.92      2142\n",
      "   macro avg       0.91      0.92      0.91      2142\n",
      "weighted avg       0.92      0.92      0.92      2142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 4. Modelo 3: Gradient Boosting (tercer método)\n",
    "# ===========================\n",
    "gb_clf = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "param_grid_gb = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [3, 5]\n",
    "}\n",
    "\n",
    "grid_gb = GridSearchCV(\n",
    "    gb_clf,\n",
    "    param_grid_gb,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Entrenando GradientBoosting (GridSearchCV)...\")\n",
    "grid_gb.fit(X_train, y_train)\n",
    "print(\"Mejores parámetros GB:\", grid_gb.best_params_)\n",
    "print(\"Mejor AUC CV GB:\", grid_gb.best_score_)\n",
    "\n",
    "best_gb = grid_gb.best_estimator_\n",
    "y_pred_gb = best_gb.predict(X_test)\n",
    "y_proba_gb = best_gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print_metrics(\"GRADIENT BOOSTING TUNED\", y_test, y_pred_gb, y_proba_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd36c79",
   "metadata": {},
   "source": [
    "## Calibración de probabilidades: Platt vs Isotónica sobre Random Forest\n",
    "\n",
    "Finalmente, realizamos un **análisis de calibración** sobre el modelo seleccionado (Random Forest):\n",
    "\n",
    "- Usamos `CalibratedClassifierCV` con dos métodos:\n",
    "  - **Platt (sigmoid)**: ajuste logístico sobre las probabilidades.\n",
    "  - **Isotónica**: calibración no paramétrica más flexible.\n",
    "- Entrenamos ambos calibradores sobre el conjunto de entrenamiento (`X_train`, `y_train`).\n",
    "- Evaluamos las probabilidades calibradas en el conjunto de prueba mediante:\n",
    "  - **Brier score** (mientras más bajo, mejor calibración).\n",
    "  - **AUC** (para verificar que no se pierda demasiado desempeño).\n",
    "\n",
    "Este análisis permite justificar cuál variante del modelo está **mejor calibrada** y es más adecuada para escenarios donde la probabilidad predicha es crítica (por ejemplo, priorizar correos a revisar manualmente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5008e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Calibrando RandomForest (Platt / Isotonic)...\n",
      "\n",
      "Brier score RF sin calibrar : 0.06233317159808155\n",
      "Brier score RF Platt (sigmoid): 0.0617109837806013\n",
      "Brier score RF Isotonic       : 0.06082338850997491\n",
      "AUC RF Platt : 0.9720028019347746\n",
      "AUC RF Iso   : 0.9720646364637295\n",
      "\n",
      " Modelado tuneado + calibración COMPLETO.\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 5. Calibración (Platt vs Isotónica) sobre el mejor modelo\n",
    "#    Aquí tomamos el RANDOM FOREST como candidato.\n",
    "# ===========================\n",
    "print(\"\\n>>> Calibrando RandomForest (Platt / Isotonic)...\")\n",
    "\n",
    "cal_platt = CalibratedClassifierCV(best_rf, method=\"sigmoid\", cv=5)\n",
    "cal_platt.fit(X_train, y_train)\n",
    "\n",
    "cal_iso = CalibratedClassifierCV(best_rf, method=\"isotonic\", cv=5)\n",
    "cal_iso.fit(X_train, y_train)\n",
    "\n",
    "y_proba_platt = cal_platt.predict_proba(X_test)[:, 1]\n",
    "y_proba_iso   = cal_iso.predict_proba(X_test)[:, 1]\n",
    "\n",
    "brier_platt = brier_score_loss(y_test, y_proba_platt)\n",
    "brier_iso   = brier_score_loss(y_test, y_proba_iso)\n",
    "\n",
    "print(\"\\nBrier score RF sin calibrar :\", brier_score_loss(y_test, y_proba_rf))\n",
    "print(\"Brier score RF Platt (sigmoid):\", brier_platt)\n",
    "print(\"Brier score RF Isotonic       :\", brier_iso)\n",
    "print(\"AUC RF Platt :\", roc_auc_score(y_test, y_proba_platt))\n",
    "print(\"AUC RF Iso   :\", roc_auc_score(y_test, y_proba_iso))\n",
    "\n",
    "print(\"\\n Modelado tuneado + calibración COMPLETO.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
